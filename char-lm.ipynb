{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Importing libraries\n",
    "include('init.lua')\n",
    "include('WordIndexer.lua')\n",
    "include('BatchIterator.lua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading indexer and data ...\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Total: 27884\t\n",
       "Train: 22307\t\n",
       "Valid: 2788\t\n",
       "Test: 2789\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Loading input data\n",
    "local textfile = 'data/tiny_shakespeare.txt'\n",
    "local indexfile = 'data/tiny_shakespeare.indexer.th'\n",
    "local datafile = 'data/tiny_shakespeare.data.th'\n",
    "\n",
    "local train_split = 0.8\n",
    "local valid_split = 0.1\n",
    "\n",
    "seq_len = 40\n",
    "\n",
    "if paths.filep(indexfile) and paths.filep(datafile) then\n",
    "    print('Loading indexer and data ...')\n",
    "    indexer = torch.load(indexfile)\n",
    "    data = torch.load(datafile)\n",
    "else\n",
    "    -- Building vocab\n",
    "    print('Building vocab...')\n",
    "    indexer = WordIndexer()\n",
    "    local data_len = 0\n",
    "    local f = assert(io.open(textfile, \"r\"))\n",
    "    while true do\n",
    "        local line = f:read()\n",
    "        if not line then break end\n",
    "        for c in line:gmatch('.') do\n",
    "            indexer:add(c)\n",
    "        end\n",
    "        data_len = data_len + #line + 1\n",
    "    end\n",
    "    f:close()\n",
    "    indexer:add('\\n')\n",
    "    print('Total chars: ' .. data_len)\n",
    "    print('Total vocab: ' .. #indexer)\n",
    "\n",
    "    -- Creating torch tensor\n",
    "    print('Creating torch Tensor of data...')\n",
    "    data = torch.ByteTensor(data_len)\n",
    "    local cur_pos = 1\n",
    "    local f = assert(io.open(textfile, \"r\"))\n",
    "    while true do\n",
    "        local line = f:read()\n",
    "        if not line then break end\n",
    "        for c in line:gmatch('.') do\n",
    "            data[cur_pos] = indexer:index(c)\n",
    "            cur_pos = cur_pos + 1\n",
    "        end\n",
    "        data[cur_pos] = indexer:index('\\n')\n",
    "        cur_pos = cur_pos + 1\n",
    "    end\n",
    "    f:close()\n",
    "    -- Saving preprocessed data for later\n",
    "    print('Saving data...')\n",
    "    torch.save(indexfile, indexer)\n",
    "    torch.save(datafile, data)\n",
    "end\n",
    "-- creating training batch\n",
    "local len = data:size(1)\n",
    "if len % (seq_len) ~= 0 then\n",
    "    data = data:sub(1, seq_len * math.floor(len / seq_len))\n",
    "end\n",
    "\n",
    "labels = data:clone()\n",
    "labels:sub(1,-2):copy(data:sub(2,-1))\n",
    "labels[-1] = data[1]\n",
    "data_seqs = data:split(seq_len)\n",
    "label_seqs = labels:split(seq_len)\n",
    "ntrain = math.floor(#data_seqs * train_split)\n",
    "nvalid = math.floor(#data_seqs * valid_split)\n",
    "ntest = #data_seqs - ntrain - nvalid\n",
    "collectgarbage()\n",
    "print('Total: ' .. #data_seqs)\n",
    "print('Train: ' .. ntrain)\n",
    "print('Valid: ' .. nvalid)\n",
    "print('Test: ' .. ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- constructing model\n",
    "dim_word = 32\n",
    "num_lstm_layers = 1\n",
    "dim_cell = 32\n",
    "dim_w = 16\n",
    "\n",
    "emb = nn.LookupTable(#indexer, dim_word)\n",
    "net = lstm.LSTM({input_dim=dim_word, hidden_dim=dim_cell, num_layers=num_lstm_layers})\n",
    "classifier = nn.Sequential()\n",
    "classifier:add(nn.Linear(dim_cell, dim_w))\n",
    "classifier:add(nn.ReLU())\n",
    "classifier:add(nn.Linear(dim_w, #indexer))\n",
    "classifier:add(nn.LogSoftMax())\n",
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local batch_size = 20\n",
    "local n_epochs = 1\n",
    "\n",
    "local mb_data = torch.zeros(batch_size, seq_len)\n",
    "local mb_labels = torch.zeros(batch_size, seq_len)\n",
    "local set_minibatch_data = function(mb_idx)\n",
    "    for i = 1, mb_idx:size(1) do\n",
    "        mb_data[{i, {}}] = data_seqs[i]\n",
    "        mb_labels[{i, {}}] = label_seqs[i]\n",
    "    end\n",
    "end\n",
    "for epoch = 1, n_epochs do\n",
    "    -- setting up data for this epoch\n",
    "    local shuffle = torch.randperm(ntrain)\n",
    "    local batch_iter = BatchIterator(shuffle, batch_size, true)\n",
    "    while batch_iter:has_next() do\n",
    "        -- mini batch\n",
    "        set_minibatch_data(batch_iter:next_batch())\n",
    "        rep = emb:forward(mb_data)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 20\n",
       " 40\n",
       " 32\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rep:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
